# fetch_news_rss.py
import logging
import os
from datetime import datetime

import feedparser
import pandas as pd
import yaml

# from utils.cache import RedisCache, fetch_url_with_cache
from scores_news.utils.cache import RedisCache, fetch_url_with_cache

logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s: %(message)s'
)
logger = logging.getLogger(__name__)

def load_sentiment_feeds(config_path=r"C:\Users\inter\PycharmProjects\FuturesMarketAI\scores_news\config\sources.yaml"):
    """×˜×•×¢×Ÿ ×§×™×©×•×¨×™ RSS ×œ×¤×™ ×§×˜×’×•×¨×™×™×ª sentiment"""
    with open(config_path, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    return config["rss_feeds"]["sentiment"]

def fetch_feed_articles(feed_url, cache, timeout=30, retries=1):
    """××©×™×›×ª ×›×ª×‘×•×ª ×Ö¾RSS ×›×•×œ×œ ×©×™××•×© ×‘Ö¾Redis Cache"""

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/124.0.0.0 Safari/537.36"
        )
    }

    try:
        raw_feed = fetch_url_with_cache(feed_url, cache, headers=headers, timeout=timeout, retries=retries)
        feed = feedparser.parse(raw_feed)
    except Exception as e:
        logger.error(f"Error fetching feed {feed_url}: {e}")
        return []

    articles = []
    for entry in feed.entries:
        articles.append({
            "title": entry.get("title", ""),
            "summary": entry.get("summary", ""),
            "link": entry.get("link", ""),
            "published": entry.get("published", ""),
            "source": feed_url
        })

    return articles

def fetch_all_sentiment_articles():
    """××©×™×›×ª ×›×œ ×”×›×ª×‘×•×ª ××›×œ ×¤×™×“×™ ×”×¡× ×˜×™×× ×˜ ×¢× Cache"""
    feeds = load_sentiment_feeds()
    all_articles = []

    cache = RedisCache(ttl_seconds=600)  # Cache ×œÖ¾10 ×“×§×•×ª

    for url in feeds:
        logger.info(f"ğŸ“¡ ×˜×•×¢×Ÿ RSS: {url}")
        try:
            articles = fetch_feed_articles(url, cache)
            logger.info(f"âœ… {len(articles)} ×›×ª×‘×•×ª ×Ö¾{url}")
            all_articles.extend(articles)
        except Exception as e:
            logger.warning(f"âš ï¸ ×©×’×™××” ×‘×”×‘××ª × ×ª×•× ×™× ×: {url} â€“ {e}")

    return pd.DataFrame(all_articles)

def fetch_and_save_sentiment_data(date=None):
    """×¤×•× ×§×¦×™×” ×¦×™×‘×•×¨×™×ª ×œ×©×™××•×© ×’× ××‘×—×•×¥"""
    if not date:
        date = datetime.now().strftime("%Y-%m-%d")

    df = fetch_all_sentiment_articles()

    if df.empty:
        logger.warning("âš ï¸ ×œ× × ××¦××• ×›×ª×‘×•×ª. ×§×•×‘×¥ ×œ× ×™×™×©××¨.")
        return

    output_dir = r"/AgentMarket/scores_news/logs"
    os.makedirs(output_dir, exist_ok=True)

    output_path = os.path.join(output_dir, f"sentiment_raw_{date}.csv")
    df.to_csv(output_path, index=False)
    logger.info(f"âœ… × ×©××¨×• {len(df)} ×›×ª×‘×•×ª ××”×§×˜×’×•×¨×™×” 'sentiment' ××œ ×”×§×•×‘×¥: {output_path}")

# ×§×¨×™××” ×™×©×™×¨×”
if __name__ == "__main__":
    try:
        fetch_and_save_sentiment_data()
    except Exception as e:
        logger.error(f"âŒ ×©×’×™××ª ×¨×™×¦×”: {e}")
